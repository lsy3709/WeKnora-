## 소개
WeKnora는 생산 환경에 즉시 투입 가능한 엔터프라이즈급 RAG(Retrieval-Augmented Generation) 프레임워크로, 지능형 문서 이해 및 검색 기능을 구현합니다. 이 시스템은 모듈식 설계를 채택하여 문서 이해, 벡터 저장, 추론 엔진 등의 기능을 분리했습니다.

![arc](./images/arc.png)

---

## 파이프라인 (Pipeline)
WeKnora는 문서를 처리하기 위해 **삽입 -> 지식 추출 -> 인덱싱 -> 검색 -> 생성**의 여러 단계를 거칩니다. 전체 프로세스는 다양한 검색 방법을 지원합니다.

![](./images/pipeline2.jpeg)

사용자가 업로드한 숙박 내역서 PDF 파일을 예로 들어 데이터 흐름을 자세히 설명합니다:

### 1. 요청 수신 및 초기화 
+ **요청 식별**: 시스템은 요청을 수신하고 전체 처리 과정을 추적하기 위해 고유한 `request_id=Lkq0OGLYu2fV`를 할당합니다.
+ **테넌트 및 세션 검증**:
    - 먼저 테넌트 정보(ID: 1, Name: Default Tenant)를 검증합니다.
    - 이어서 세션 `1f241340-ae75-40a5-8731-9a3a82e34fdd`에 속한 지식베이스 질의응답(Knowledge QA) 요청 처리를 시작합니다.
+ **사용자 질문**: 사용자의 원본 질문은 "**입주한 객실 타입은 무엇인가요?**"입니다.
+ **메시지 생성**: 사용자의 질문과 생성될 답변에 대해 각각 메시지 레코드를 생성합니다 (ID: `703ddf09-...` 및 `6f057649-...`).

### 2. 지식베이스 질의응답 프로세스 시작
시스템은 공식적으로 지식베이스 질의응답 서비스를 호출하고, 순차적으로 실행될 전체 처리 파이프라인을 정의합니다. 여기에는 다음 9가지 이벤트가 포함됩니다:  
`[rewrite_query, preprocess_query, chunk_search, chunk_rerank, chunk_merge, filter_top_k, into_chat_message, chat_completion_stream, stream_filter]`

---

### 3. 이벤트 실행 상세
#### 이벤트 1: `rewrite_query` - 질문 재작성
+ **목적**: 검색을 더 정확하게 하기 위해 시스템은 문맥을 결합하여 사용자의 실제 의도를 이해해야 합니다.
+ **작업**:
    1. 현재 세션의 최근 20개 기록(실제 8개 검색됨)을 문맥으로 가져옵니다.
    2. `deepseek-r1:7b`라는 로컬 대규모 언어 모델을 호출합니다.
    3. 모델은 대화 기록을 바탕으로 질문자가 "Liwx"임을 분석하고, 원본 질문을 더 구체적으로 재작성합니다.
+ **결과**: 질문이 "**Liwx의 이번 숙박 객실 타입은 무엇인가요?**"로 성공적으로 재작성되었습니다.

#### 이벤트 2: `preprocess_query` - 질문 전처리
+ **목적**: 재작성된 질문을 형태소 분석하여 검색 엔진 처리에 적합한 키워드 시퀀스로 변환합니다.
+ **작업**: 재작성된 질문에 대해 단어 분리 처리를 수행합니다.
+ **결과**: 키워드 시퀀스가 생성됩니다.

#### 이벤트 3: `chunk_search` - 지식 청크 검색
이 단계는 가장 핵심적인 **검색(Retrieval)** 단계로, 시스템은 두 번의 하이브리드 검색(Hybrid Search)을 수행합니다.

+ **첫 번째 검색 (재작성된 전체 문장 사용)**:
    - **벡터 검색**:
        1. 임베딩 모델 `bge-m3:latest`를 로드하여 문장을 1024차원 벡터로 변환합니다.
        2. PostgreSQL 데이터베이스에서 벡터 유사도 검색을 수행하여 2개의 관련 지식 청크(ID: `e3bf6599-...` 및 `3989c6ce-...`)를 찾습니다.
    - **키워드 검색**:
        1. 동시에 키워드 검색도 수행합니다.
        2. 동일한 2개의 지식 청크를 찾습니다.
    - **결과 병합**: 두 방법으로 찾은 결과에서 중복을 제거하여 최종적으로 2개의 고유한 청크를 얻습니다.
+ **두 번째 검색 (전처리된 키워드 시퀀스 사용)**:
    - 단어 분리된 키워드를 사용하여 위와 동일한 **벡터 검색** 및 **키워드 검색** 과정을 반복합니다.
+ **최종 결과**: 두 번의 검색과 결과 병합을 통해 시스템은 가장 관련성이 높은 2개의 청크를 확정하고 답변 생성을 위해 내용을 추출합니다.

#### 이벤트 4: `chunk_rerank` - 결과 재정렬 
+ **목적**: 더 강력한 모델을 사용하여 초기 검색 결과를 더 정교하게 정렬함으로써 최종 답변의 품질을 높입니다.
+ **작업**: 로그에 `Rerank model ID is empty, skipping reranking`이 표시되었습니다. 이는 재정렬 단계가 구성되었으나 구체적인 모델이 지정되지 않아 **이 단계를 건너뛰었음**을 의미합니다.

#### 이벤트 5: `chunk_merge` - 청크 병합
+ **목적**: 내용상 인접하거나 관련된 지식 청크를 병합하여 더 완전한 문맥을 형성합니다.
+ **작업**: 검색된 2개의 청크를 분석하고 병합을 시도합니다. 관련성 점수에 따라 정렬된 상태를 유지합니다.

#### 이벤트 6: `filter_top_k` - Top-K 필터링 
+ **목적**: 가장 관련성이 높은 K개의 결과만 남겨 언어 모델에 불필요한 정보가 입력되는 것을 방지합니다.
+ **작업**: 상위 5개(Top-K = 5)의 청크만 남기도록 설정되었습니다. 현재 2개뿐이므로 모두 통과합니다.

#### 이벤트 7 & 8: `into_chat_message` & `chat_completion_stream` - 답변 생성
이 단계는 **생성(Generation)** 단계입니다.

+ **목적**: 검색된 정보를 기반으로 자연스럽고 유창한 답변을 생성합니다.
+ **작업**:
    1. 검색된 2개의 청크 내용, 사용자의 원본 질문, 대화 기록을 통합하여 완전한 프롬프트(Prompt)를 구성합니다.
    2. 다시 `deepseek-r1:7b` 모델을 호출하여 **스트리밍(Stream)** 방식으로 답변 생성을 요청합니다.

#### 이벤트 9: `stream_filter` - 스트리밍 출력 필터링
+ **목적**: 모델이 생성하는 실시간 텍스트 흐름을 후처리하여 불필요한 특수 마커나 내용을 걸러냅니다.
+ **작업**: 모델의 사고 과정에서 발생하는 `<think>` 및 `</think>` 태그를 제거하는 필터를 적용합니다.

### 4. 완료 및 응답 
+ **참조 전송**: 답변 생성과 동시에 근거가 된 2개의 지식 청크를 "참조 내용"으로 프론트엔드에 전송하여 사용자가 출처를 확인할 수 있게 합니다.
+ **메시지 업데이트**: 모델 생성이 완료되면 전체 답변을 메시지 레코드에 업데이트합니다.
+ **요청 종료**: 서버가 `200` 성공 코드를 반환하며 질문부터 답변까지의 전체 프로세스가 종료됩니다.

### 요약
이 과정은 전형적인 RAG 워크플로우를 보여줍니다: **질문 재작성**과 **전처리**를 통해 의도를 정확히 이해하고, **벡터 및 키워드 하이브리드 검색**으로 관련 정보를 찾은 뒤, 이를 문맥으로 사용하여 대규모 언어 모델이 정확한 답변을 **생성**합니다.

---

## 문서 파싱 및 분할
독립적인 gRPC 통신 마이크로서비스가 문서 내용의 심층 파싱, 청킹 및 멀티모달 정보 추출을 담당합니다. 이는 비동기 처리 단계의 핵심 실행자입니다.

### **전체 아키텍처**
Python 기반의 gRPC 서비스로, 파일(또는 URL)을 수신하여 후속 처리(벡터화 등)가 가능한 구조화된 텍스트 블록(Chunks)으로 파싱하는 것이 핵심 역할입니다.

+ `server.py`: 서비스의 진입점 및 네트워크 계층. 멀티프로세스/멀티스레드 gRPC 서버를 실행하여 Go 백엔드의 요청을 처리합니다.
