# 02. 환경 설정: 모델 및 서비스 구성

WeKnora의 AI 기능을 활성화하기 위해 LLM 및 임베딩 모델을 설정하는 방법을 설명합니다.

## 1. 초기화 마법사 (첫 접속 시)
계정 생성 후 첫 접속 시 시스템 초기화 마법사가 나타날 수 있습니다. 안내에 따라 다음 모델들을 구성합니다.

## 2. 전역 모델 관리
왼쪽 메뉴 하단의 **"시스템 설정" (Settings)** -> **"모델 관리" (Model Management)** 탭에서 모델을 추가할 수 있습니다.

### LLM 모델 설정 (대화용)
- **Ollama (로컬)**: 로컬에 설치된 Ollama 서비스를 통해 모델을 사용합니다.
- **Remote API (원격)**: OpenAI, DeepSeek, Anthropic 등 외부 API를 연동합니다.
- **필수 입력**: 모델 이름, Base URL, API Key(원격인 경우).

### 임베딩 모델 설정 (문서 벡터화용)
- 문서를 수치화하여 저장할 때 사용되는 모델입니다.
- **차원(Dimension)** 확인: 모델에 맞는 벡터 차원(예: 768, 1536)을 정확히 입력해야 합니다. "차원 감지" 기능을 활용하면 편리합니다.

## 3. 서비스 상태 확인
- **Ollama 설정**: 로컬 모델을 사용한다면 Ollama 서비스가 "사용 가능" 상태인지 확인합니다.
- **검색 엔진**: 웹 검색 기능을 사용하려면 Google, Bing 또는 DuckDuckGo 등의 제공업체 설정을 완료해야 합니다.

---
**다음 단계**: 모델 설정이 완료되었다면 [03. 지식 관리: 지식베이스 및 문서 업로드](./03_지식_관리_지식베이스_및_문서_업로드.md)를 통해 데이터를 학습시켜 보세요.
